{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"smith-test\"\n",
    "\n",
    "# The below examples use the OpenAI API, so you will need\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\"What is Arize AI?\", \"Arize AI is an ML Observability Platform designed for real-time monitoring, analysis, and explainability of machine learning models.\"),\n",
    "    (\"What are the main uses of Arize for ML practitioners?\", \"ML practitioners use Arize to evaluate, monitor, and troubleshoot LLM applications, monitor real-time model performance, root cause model failures/performance degradation, conduct multi-model performance comparisons, and surface drift, data quality, and model fairness/bias metrics.\"),\n",
    "    (\"What does the Arize platform log?\", \"The Arize platform logs model inferences across training, validation, and production environments.\"),\n",
    "    (\"How does Arize fit into the ML Stack?\", \"Arize fits into the ML Stack by providing ML Observability, which offers a deep understanding of a model's performance and the reasons behind its behavior, especially after the models are deployed into production.\"),\n",
    "    (\"What components might an ML Stack include?\", \"An ML Stack might include a feature store, model store, and serving layer, along with an inference/evaluation store for post-deployment model performance insights.\"),\n",
    "    (\"Is Arize platform and model agnostic?\", \"Yes, Arize is platform and model agnostic, meaning it can work with various machine learning infrastructures and can be deployed as SaaS or in other forms.\"),\n",
    "    (\"What is ML Observability?\", \"ML Observability is the practice of obtaining a deep understanding into your modelâ€™s data and performance across its lifecycle. It involves more than just indicating if the model is working correctly; it enables ML practitioners to understand and explain why a model is behaving a certain way to improve its performance.\"),\n",
    "    (\"What resources are available for best practices in ML Observability?\", \"Arize offers resources and guidance on best practices in ML Observability. These resources help ML practitioners understand how to effectively monitor and analyze their models throughout the ML lifecycle, ensuring they get the most out of ML Observability.\"),\n",
    "    (\"How do you set up the Python SDK for Arize?\", \"First, install the Arize SDK using 'pip install arize'. Then, initialize the Arize client by importing the Client and Schema from arize.pandas.logger and setting up your API and space keys.\"),\n",
    "    (\"How do you define a model schema in Arize?\", \"Define a model schema by specifying required and optional parameters, such as prediction ID, timestamp, labels, features, and tags. Optional parameters can include embeddings, SHAP values, and delayed actuals.\"),\n",
    "    (\"How do you log inferences to Arize using the Python SDK?\", \"Log inferences by ensuring the DataFrame's index is sorted and begins at 0. Use the arize_client.log() method to send the DataFrame along with model details like model_id, model_version, and schema to Arize.\"),\n",
    "    (\"What are the optional features when setting a schema in Arize SDK?\", \"Optional schema features include embeddings, SHAP values, and delayed actuals. Embeddings require vector, text, and image link columns. SHAP values are logged with corresponding feature columns, and delayed actuals are logged using the same prediction ID.\"),\n",
    "    (\"What is the purpose of metrics validation in Arize SDK?\", \"Metrics validation, an optional argument, specifies desired metric groups for validation. It helps ensure that the expected metrics will be available on the platform and validates the required schema columns based on the model type.\"),\n",
    "    (\"What is a Model Schema in Arize?\", \"A Model Schema in Arize organizes model data, including inputs (features), outputs (predictions), timestamps, ground truth (actuals), metadata (tags), and internals (embeddings/SHAP). It varies by data ingestion method and model type.\"),\n",
    "    (\"How do you define an Example Schema in Arize?\", \"\"\"An Example Schema in Arize can include prediction IDs, timestamps, labels, scores, features, tags, embeddings, and URLs. It's defined using the Schema class, specifying column names for various model attributes like prediction ID, features, tags, timestamps, labels, scores, and optional embeddings and SHAP values. Here's a snippet to define a schema:\n",
    "```python\n",
    "schema = Schema(\n",
    "    prediction_id_column_name=\"prediction id\",\n",
    "    feature_column_names=[\"feature_1\", \"feature_2\", \"feature_3\"],\n",
    "    tag_column_names=[\"tag_1\", \"tag_2\", \"tag_3\"],\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"prediction_label\",\n",
    "    prediction_score_column_name=\"prediction_score\",\n",
    "    actual_label_column_name=\"actual_label\",\n",
    "    actual_score_column_name=\"actual_score\",\n",
    "    shap_values_column_names=shap_values_column_names=dict(zip(\"feature_1\", shap_cols)),\n",
    "    embedding_feature_column_names=embedding_feature_column_names,\n",
    "    prediction_group_id_column_name=\"group_id_name\",\n",
    "    rank_column_name=\"rank\",\n",
    "    relevance_score_column_name=\"relevance_score\",\n",
    "    relevance_labels_column_name=\"actual_relevancy\",\n",
    ")\n",
    "\"\"\"),\n",
    "(\"What are the key components of a model schema?\", \"Key components include Model Name, Model Version, Model Environments, Model Type, Prediction ID, Timestamp, Features, Embedding Features, and Tags. Each plays a crucial role in organizing and understanding the model's data and performance.\"),\n",
    "(\"How do you log data with Arize?\", \"\"\"To log data with Arize, define your schema and use the arize.log() function with parameters like dataframe, schema, environment, model_id, model_type, metrics_validation, model_version, and validate set to True. Here's an example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "response = arize.log(\n",
    "    dataframe=df,\n",
    "    schema=schema,\n",
    "    environment=Environments.Production,\n",
    "    model_id=\"example_model\",\n",
    "    model_type=ModelTypes.BINARY_CLASSIFICATION,\n",
    "    metrics_validation=[Metrics.CLASSIFICATION, Metrics.REGRESSION, Metrics.AUC_LOG_LOSS],\n",
    "    model_version=\"1.0\",\n",
    "    validate=True\n",
    ")\n",
    "\"\"\"),\n",
    "(\"What are Delayed (Latent) Actuals?\", \"Delayed actuals refer to the ground truth data collected after a delay in the feedback loop for a model's predictions. Arize can automatically connect these actuals to earlier predictions using the same prediction ID.\"),\n",
    "(\"How do you send Delayed Actuals to Arize?\", \"To send delayed actuals, use the same prediction_id for actuals as the corresponding predictions. Arize's joiner, which runs daily, matches these actuals with predictions within a 14-day window, extendable upon request.\"),\n",
    "(\"What is the Arize joiner and how does it work?\", \"The Arize joiner automatically maps delayed actuals to their corresponding predictions daily at 05:00 UTC, with a default lookback window of 14 days. This process supports all data upload methods and is based on the prediction_id.\"),\n",
    "(\"What are the requirements for joining delayed actuals in Arize?\", \"Joining requires the prediction_id to match between actuals and predictions, and the actual_score/label to be provided. The model_id is also required to ensure actuals match the correct model.\"),\n",
    "(\"How can you ensure delayed actuals match predictions in Arize?\", \"Ensure your prediction ID, model name, and space match between your predictions and actuals when defining the schema for data ingestion jobs. Arize will then automatically sync the new data.\"),\n",
    "(\"How do tags work with delayed actuals in Arize?\", \"Tags can be updated with delayed actuals. If tags are sent with actuals, they will be joined based on the prediction_id. However, if an actual is resent with an updated tag, the new tag value will not update the existing tag.\"),\n",
    "(\"How does Arize measure model performance with delayed actuals?\", \"Arize calculates performance metrics only for predictions that have matched actuals. For predictions awaiting actuals, use other performance metrics to monitor model health.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "dataset_name = f\"Arize Docs QA Questions {str(uuid.uuid4())}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "for q, a, in examples:\n",
    "    client.create_example(inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.arize.com/arize/\")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "\n",
    "\n",
    "def create_retriever(transformed_documents, text_splitter):\n",
    "    documents = text_splitter.split_documents(transformed_documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "                \" questions from Arize's documentation.\"\n",
    "                \" Arize is a machine learing observability platform.\"\n",
    "                \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "                (\"system\", \"{context}\"),\n",
    "                (\"human\",\"{question}\")\n",
    "            ]\n",
    "        ).partial(time=str(datetime.datetime.now()))\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    response_generator = (\n",
    "        prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    chain = (\n",
    "        # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "retriever = create_retriever(transformed, text_splitter)\n",
    "\n",
    "chain_1 = create_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_2 = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "retriever_2 = create_retriever(transformed,text_splitter_2)\n",
    "\n",
    "chain_2 = create_chain(retriever_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
