{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Demo\n",
    "    By Aaron Roberts\n",
    "\n",
    "* LangSmith is a platform for building production-grade LLM applications. URL: https://smith.langchain.com/\n",
    "* It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework\n",
    "* It seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.\n",
    "\n",
    "    ** It is in a private Beta and need to be added to wait list (I got an account almost instantly) **\n",
    "\n",
    "### Set Environment Variables\n",
    "All you need to do in order to use LangSmith is to set a number of Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#LangSmith API keys:\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY') # API key on LangSmith\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true' # Enable Tracing\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\" # Cloud end point\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"LangSmith-Demo\" # Project name\n",
    "\n",
    "# Other API KEYS:\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['SERPAPI_API_KEY'] = os.getenv('SERPAPI_API_KEY') # For internet searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Basic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter \n",
    "\n",
    "from langchain import hub #This is a prompt repository\n",
    "from langchain.agents import AgentExecutor, load_tools #Loads an Agent with tool capabilites, a way to load in tools by name\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions # Convert Agent Actions and Tool outputs into function messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser # Parses a message into an Agent Action or the final output\n",
    "from langchain.chat_models import ChatOpenAI #This is the basic chat API for OpenAI\n",
    "from langchain.tools.render import format_tool_to_openai_function #Converts tool to match the openAI function\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) # Creates a llm connection 3.5 turbo by default\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) # Load in the tools\n",
    "llm_with_tools = llm.bind(functions = [format_tool_to_openai_function(t) for t in tools]) # Adding the tools as OpenAI functions\n",
    "\n",
    "prompt = hub.pull(\"wfh/agent-lcel-prompt\") # Pulls a prompt from langhub that works well with agents with tools\n",
    "\n",
    "\n",
    "#Builds out the agent using LCEL (langChain expression language)\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "# Build the Agent RunTime\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Some Inputs for the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the number of cats in the US multiplied by the number of cats in China\",\n",
    "    \"How much is all of the cats in the world multiplied by the number of the number of legs that a cat has?\",\n",
    "    \"How many cats are there in the US divided by the number of US citizens?\",\n",
    "    \"How much larger is an average cat to an average dog?\",\n",
    "    \"Are there more dogs or cats in the US?\",\n",
    "    \"What is the size ratio from the largest cat to the smallest cat?\",\n",
    "    \"How many cats does it take to fill a 1500 square foot house?\",\n",
    "    \"How many more hours per day do cats sleep than the average human?\",\n",
    "    \"How many cat themed Gods are there compared to dog themed Gods?\",\n",
    "    \"Is the largest rat bigger than the smallest cat?\",\n",
    "    \"How many cats are there in the United States to the power of 10?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the inputs and send them langsmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for single inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many cats are there in the state of Texas to the power of 10?',\n",
       " 'output': 'There are 10 billion cats in the state of Texas to the power of 10.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Calculator', tool_input='10^10', log='\\nInvoking: `Calculator` with `10^10`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"10^10\"\\n}', 'name': 'Calculator'}})]),\n",
       "   'Answer: 10000000000')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"How many cats are there in the state of Texas to the power of 10?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LangChain Traces are sent to the Project View\n",
    "\n",
    "##### You can view the different stats of the run.\n",
    "\n",
    "* Endstate status (complete/error)\n",
    "* Latency\n",
    "* Tokens\n",
    "* Aggregate Stats\n",
    "\n",
    "##### Tree View / Agent Path\n",
    "\n",
    "* Step Through the Tree Trace\n",
    "* Keeps track of Tools\n",
    "* Steps through each step (Search/Math)\n",
    "* Keeps track of the response in the agent scratch pad\n",
    "* Reasons for the next steps.\n",
    "* Get the break down of token usage/latency at each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith Playground\n",
    "\n",
    "* Try a different model\n",
    "* Change the prompt\n",
    "* Inputs\n",
    "* Temperature\n",
    "* **Need to supply API key, LangSmith calls the API on your behalf.**\n",
    "* Easy way to test out the flow of the agent to see if you get a different results.\n",
    "* Does not change the code, but a good place test things out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy to check where the error exist in the Agent Run.\n",
    "\n",
    "* Very useful way to check for problems.\n",
    "* Can find the exact problem and can adjust code to include gaurd rails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing the Run (Collaborative Debugging)\n",
    "\n",
    "* The run can be shared to anyone with the link regardless if they have LangSmith or not.\n",
    "* Great way to get someone to help narrow down a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collecting examples**\n",
    "\n",
    "* Very easy to collect datasets\n",
    "* Hard to collect datasets (You need thousands to 10s of thousands of datasets)\n",
    "* Datasets can be golden datasets (ideal responses)\n",
    "* Problem Datasets (Might want to iterate over these to find ways to prevent the underlying issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting a Golden Dataset\n",
    "\n",
    "* Filter for the successful runs.\n",
    "* Check next to the runs that you found to be correct\n",
    "* Create a new dataset\n",
    "\n",
    "#### There are many ways to create datasets\n",
    "\n",
    "* Manually\n",
    "* Upload a CSV\n",
    "* Do it programmatically\n",
    "\n",
    "**You can share your dataset to your team**\n",
    "\n",
    "Collecting Datasets is the first thing that you need to do in order to do further evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When putting your LLM into production you need to test the model over a vast number of prompts to be confident that it meets all of your standards before releasing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf evaluators\n",
    "\n",
    "* Correctness\n",
    "* Helpfullness\n",
    "* Harmfullness\n",
    "\n",
    "* You can build your own tools (Check for sql or json formatting)\n",
    "* Lots of the hard work! Build competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-monthly-osmosis-17' at:\n",
      "https://smith.langchain.com/o/ca43bddc-1333-5cd7-840e-ed9a34e1ed28/datasets/132d4afb-32ce-4d5d-a91f-e0af47440efd/compare?selectedSessions=080c7b69-1262-44b6-8046-195f2b7699ac\n",
      "\n",
      "View all tests for Dataset Test-successful-dataset at:\n",
      "https://smith.langchain.com/o/ca43bddc-1333-5cd7-840e-ed9a34e1ed28/datasets/132d4afb-32ce-4d5d-a91f-e0af47440efd\n",
      "[>                                                 ] 0/12"
     ]
    }
   ],
   "source": [
    "import langsmith\n",
    "\n",
    "from langchain import chat_models, prompts, smith\n",
    "from langchain.schema import output_parser\n",
    "\n",
    "\n",
    "eval_config = smith.RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"cot_qa\",\n",
    "        smith.RunEvalConfig.LabeledCriteria(\"harmfulness\"),\n",
    "        smith.RunEvalConfig.LabeledCriteria(\"helpfulness\")\n",
    "    ],\n",
    "    reference_key=\"output\",\n",
    "    eval_llm=chat_models.ChatOpenAi(model=\"gpt-4\", temperature=0)\n",
    ")\n",
    "\n",
    "client = langsmith.Client()\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=\"Test-successful-dataset\",\n",
    "    llm_or_chain_factory=agent_executor,\n",
    "    evaluation=eval_config,\n",
    "    project_name=\"test-monthly-osmosis-17\",\n",
    "    concurrency_level=5,\n",
    "    tags=[\"gpt-3.5\"] #Tags allow you to check for version control/ varients for each of your prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Result Break Down\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
