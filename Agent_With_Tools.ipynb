{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"agent-test\"\n",
    "\n",
    "# The below examples use the OpenAI API, so you will need\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    (\n",
    "        \"Why was was a $10 calculator app one of the best-rated Nintendo Switch games?\",\n",
    "        {\n",
    "            \"reference\": \"It became an internet meme due to its high price point.\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"hi\",\n",
    "        {\n",
    "            \"reference\": \"Hello, how can I assist you?\",\n",
    "            \"expected_steps\": [],  # Expect a direct response\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who is Dejan Trajkov?\",\n",
    "        {\n",
    "            \"reference\": \"Macedonian Professor, Immunologist and Physician\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who won the 2023 U23 world wresting champs (men's freestyle 92 kg)\",\n",
    "        {\n",
    "            \"reference\": \"Muhammed Gimri from turkey\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's my first meeting on Friday?\",\n",
    "        {\n",
    "            \"reference\": 'Your first meeting is 8:30 AM for \"Team Standup\"',\n",
    "            \"expected_steps\": [\"check_calendar\"],  # Only expect calendar tool\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = uuid.uuid4()\n",
    "dataset_name = f\"Agent Eval Example {uid}\"\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset using search and calendar checks.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q[0]} for q in questions],\n",
    "    outputs=[q[1] for q in questions],\n",
    "    dataset_id=ds.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import DuckDuckGoSearchResults, tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_calendar(date: str) -> list:\n",
    "    \"\"\"Check the user's calendar for a meetings on the specified datetime (in iso format).\"\"\"\n",
    "    date_time = parse(date)\n",
    "    # A placeholder to demonstrate with multiple tools.\n",
    "    # It's easy to mock tools when testing.\n",
    "    if date_time.weekday() == 4:\n",
    "        return [\n",
    "            \"8:30 : Team Standup\",\n",
    "            \"9:00 : 1 on 1\",\n",
    "            \"9:45 design review\",\n",
    "        ]\n",
    "    return [\"Focus time\"]  # If only...\n",
    "\n",
    "\n",
    "def agent_factory():\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    tools = [\n",
    "        DuckDuckGoSearchResults(\n",
    "            name=\"duck_duck_go\"\n",
    "        ),  # General internet search using DuckDuckGo\n",
    "        check_calendar,\n",
    "    ]\n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    runnable_agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"question\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_with_tools\n",
    "        | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "\n",
    "    return AgentExecutor(\n",
    "        agent=runnable_agent,\n",
    "        tools=tools,\n",
    "        handle_parsing_errors=True,\n",
    "        return_intermediate_steps=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "    def evaluate_run(\n",
    "            self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannont be None\")\n",
    "        intermediate_steps = run.outputs[\"intermediate_steps\"]\n",
    "        trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "        expected_trajectory = example.outputs[\"expected_steps\"]\n",
    "        score = int(trajectory == expected_trajectory)\n",
    "        return EvaluationResult(key=\"Intermediate steps correctness\", score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'new-sign-82' at:\n",
      "https://smith.langchain.com/o/ca43bddc-1333-5cd7-840e-ed9a34e1ed28/datasets/4f3248c5-fb75-460e-9ebd-2ca2a07ddd93/compare?selectedSessions=e93eda5b-c69e-4b21-9c0e-d54cee2e11eb\n",
      "\n",
      "View all tests for Dataset Agent Eval Example 7f387639-f6a0-498a-aa8c-b1ab11b9f9e9 at:\n",
      "https://smith.langchain.com/o/ca43bddc-1333-5cd7-840e-ed9a34e1ed28/datasets/4f3248c5-fb75-460e-9ebd-2ca2a07ddd93\n",
      "[>                                                 ] 0/5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-24' coro=<AsyncDDGS.__aexit__() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:46>>\n",
      "c:\\Python311\\Lib\\asyncio\\base_events.py:675: RuntimeWarning: coroutine 'AsyncDDGS.__aexit__' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-6' coro=<AsyncCurl._force_timeout() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\curl_cffi\\aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-19' coro=<AsyncDDGS.__aexit__() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:46>>\n",
      "c:\\Python311\\Lib\\asyncio\\base_events.py:675: RuntimeWarning: coroutine 'AsyncDDGS.__aexit__' was never awaited\n",
      "  self._ready.clear()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-29' coro=<AsyncDDGS.__aexit__() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:46>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<AsyncCurl._force_timeout() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\curl_cffi\\aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-10' coro=<AsyncCurl._force_timeout() running at C:\\Users\\THE BARON\\AppData\\Roaming\\Python\\Python311\\site-packages\\curl_cffi\\aio.py:168> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 5/5"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.Intermediate steps correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20b4af5f-37b4-4b0f-950f-e5e47e698dc0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.678820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.077773</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.204033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.688272</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.823264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.003869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness  feedback.Intermediate steps correctness error  \\\n",
       "count                    5.0                                      5.0     0   \n",
       "unique                   NaN                                      NaN     0   \n",
       "top                      NaN                                      NaN   NaN   \n",
       "freq                     NaN                                      NaN   NaN   \n",
       "mean                     0.0                                      1.0   NaN   \n",
       "std                      0.0                                      0.0   NaN   \n",
       "min                      0.0                                      1.0   NaN   \n",
       "25%                      0.0                                      1.0   NaN   \n",
       "50%                      0.0                                      1.0   NaN   \n",
       "75%                      0.0                                      1.0   NaN   \n",
       "max                      0.0                                      1.0   NaN   \n",
       "\n",
       "        execution_time                                run_id  \n",
       "count         5.000000                                     5  \n",
       "unique             NaN                                     5  \n",
       "top                NaN  20b4af5f-37b4-4b0f-950f-e5e47e698dc0  \n",
       "freq               NaN                                     1  \n",
       "mean          2.678820                                   NaN  \n",
       "std           2.077773                                   NaN  \n",
       "min           0.674660                                   NaN  \n",
       "25%           1.204033                                   NaN  \n",
       "50%           2.688272                                   NaN  \n",
       "75%           2.823264                                   NaN  \n",
       "max           6.003869                                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[AgentTrajectoryEvaluator()],\n",
    "    # We now need to specify this because we have multiple outputs in our dataset\n",
    "    reference_key=\"reference\",\n",
    ")\n",
    "\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    tags=[\"agent-eval-example\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
