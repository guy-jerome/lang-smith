{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Demo\n",
    "    By Aaron Roberts\n",
    "\n",
    "* LangSmith is a platform for building production-grade LLM applications. URL: https://smith.langchain.com/\n",
    "* It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework\n",
    "* It seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.\n",
    "\n",
    "    ** It is in a private Beta and need to be added to wait list (I got an account almost instantly) **\n",
    "\n",
    "##### Part of the presentation:\n",
    "\n",
    "* How to integrate your chains and agents with LangSmith (basic debugging)\n",
    "* Creating/ Collecting Datasets\n",
    "* Testing and Evaluations\n",
    "* Human Evaluations\n",
    "* Monitoring\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "##### Set Environment Variables\n",
    "All you need to do in order to use LangSmith is to set a number of Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#LangSmith API keys:\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY') # API key on LangSmith\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true' # Enable Tracing\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\" # Cloud endpoint\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"LangSmith-Demo\" # Project name\n",
    "\n",
    "# Other API KEYS:\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['SERPAPI_API_KEY'] = os.getenv('SERPAPI_API_KEY') # For internet searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Basic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter \n",
    "\n",
    "from langchain import hub #This is a prompt repository\n",
    "from langchain.agents import AgentExecutor, load_tools #Loads an Agent with tool capabilites, a way to load in tools by name\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions # Convert Agent Actions and Tool outputs into function messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser # Parses a message into an Agent Action or the final output\n",
    "from langchain.chat_models import ChatOpenAI #This is the basic chat API for OpenAI\n",
    "from langchain.tools.render import format_tool_to_openai_function #Converts tool to match the openAI function\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) # Creates a llm connection 3.5 turbo by default\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) # Load in the tools\n",
    "llm_with_tools = llm.bind(functions = [format_tool_to_openai_function(t) for t in tools]) # Adding the tools as OpenAI functions\n",
    "\n",
    "prompt = hub.pull(\"wfh/agent-lcel-prompt\") # Pulls a prompt from langhub that works well with agents with tools\n",
    "\n",
    "\n",
    "#Builds out the agent using LCEL (langChain expression language)\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "# Build the Agent RunTime\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Some Inputs for the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is the number of cats in the US multiplied by the number of cats in China\",\n",
    "    \"How much is all of the cats in the world multiplied by the number of the number of legs that a cat has?\",\n",
    "    \"How many cats are there in the US divided by the number of US citizens?\",\n",
    "    \"How much larger is an average cat to an average dog?\",\n",
    "    \"Are there more dogs or cats in the US?\",\n",
    "    \"What is the size ratio from the largest cat to the smallest cat?\",\n",
    "    \"How many cats does it take to fill a 1500 square foot house?\",\n",
    "    \"How many more hours per day do cats sleep than the average human?\",\n",
    "    \"How many cat themed Gods are there compared to dog themed Gods?\",\n",
    "    \"Is the largest rat bigger than the smallest cat?\",\n",
    "    \"How many cats are there in the United States to the power of 10?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the inputs and send them langsmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for single inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many cats are there in the state of Texas to the power of 10?',\n",
       " 'output': 'There are 10 billion cats in the state of Texas to the power of 10.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Calculator', tool_input='10^10', log='\\nInvoking: `Calculator` with `10^10`\\n\\n\\n', message_log=[AIMessageChunk(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"10^10\"\\n}', 'name': 'Calculator'}})]),\n",
       "   'Answer: 10000000000')]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"How many cats are there in the state of Texas to the power of 10?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LangChain Traces are sent to the Project View\n",
    "\n",
    "##### You can view the different stats of the run.\n",
    "\n",
    "* End state status (complete/error)\n",
    "* Latency\n",
    "* Tokens\n",
    "* Aggregate Stats\n",
    "\n",
    "##### Tree View / Agent Path\n",
    "\n",
    "* Step Through the Tree Trace\n",
    "* Keeps track of Tools\n",
    "* Steps through each step (Search/Math)\n",
    "* Keeps track of the response in the agent scratch pad\n",
    "* Reasons for the next steps.\n",
    "* Get the break down of token usage/latency at each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith Playground\n",
    "\n",
    "* Try a different model\n",
    "* Change the prompt\n",
    "* Inputs\n",
    "* Temperature\n",
    "* **Need to supply API key, LangSmith calls the API on your behalf.**\n",
    "* Easy way to test out the flow of the agent to see if you get a different results.\n",
    "* Does not change the code, but a good place test things out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy to check where the error exist in the Agent Run.\n",
    "\n",
    "* Very useful way to check for problems.\n",
    "* Can find the exact problem and can adjust code to include guard rails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing the Run (Collaborative Debugging)\n",
    "\n",
    "* The run can be shared to anyone with the link regardless if they have LangSmith or not.\n",
    "* Great way to get someone to help narrow down a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collecting examples**\n",
    "\n",
    "* Very easy to collect datasets\n",
    "* Hard to collect datasets (You need thousands to 10s of thousands of datasets)\n",
    "* Datasets can be golden datasets (ideal responses)\n",
    "* Problem Datasets (Might want to iterate over these to find ways to prevent the underlying issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting a Golden Dataset\n",
    "\n",
    "* Filter for the successful runs.\n",
    "* Check next to the runs that you found to be correct\n",
    "* Create a new dataset\n",
    "\n",
    "#### There are many ways to create datasets\n",
    "\n",
    "* Manually\n",
    "* Upload a CSV\n",
    "* Do it programmatically\n",
    "\n",
    "**You can share your dataset to your team**\n",
    "\n",
    "Collecting Datasets is the first thing that you need to do in order to do further evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When putting your LLM into production you need to test the model over a vast number of prompts to be confident that it meets all of your standards before releasing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf evaluators\n",
    "\n",
    "* Correctness\n",
    "* Helpfulness\n",
    "* Harmfulness\n",
    "\n",
    "* You can build your own tools (Check for sql or json formatting)\n",
    "* Lots of the hard work! Build competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "from langchain import chat_models, prompts, smith\n",
    "from langchain.schema import output_parser\n",
    "\n",
    "\n",
    "eval_config = smith.RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"cot_qa\",\n",
    "        smith.RunEvalConfig.LabeledCriteria(\"harmfulness\"),\n",
    "        smith.RunEvalConfig.LabeledCriteria(\"helpfulness\")\n",
    "    ],\n",
    "    reference_key=\"output\",\n",
    "    eval_llm=chat_models.ChatOpenAI(model=\"gpt-4\", temperature=0) # Better at evaluating responses\n",
    ")\n",
    "\n",
    "client = langsmith.Client()\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=\"demo-correct-outputs\",\n",
    "    llm_or_chain_factory=agent_executor,\n",
    "    evaluation=eval_config,\n",
    "    project_name=\"test-cot-harm-help-0.1\",\n",
    "    concurrency_level=5,\n",
    "    tags=[\"gpt-3.5\"] #Tags allow you to check for version control/ varients for each of your prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Result Break Down\n",
    "\n",
    "* You can see aggregate measures for each of the evaluation criteria for each of the tests\n",
    "* The output is being compared against the reference output from the dataset\n",
    "* Can check into the run for each of the evaluators (it is another LLM run) \n",
    "* Much easier to add evaluations\n",
    "\n",
    "#### Test Run Comparisons\n",
    "\n",
    "* You can compare your runs against each other to see the results against each other.\n",
    "* The review can see where they exactly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Human Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why would you need to have human evaluations\n",
    "\n",
    "* Evaluation criteria that is hard for a model to auto eval\n",
    "* Want a human to pick through a few examples from thousands to evaluations to make sure the LLM grader is performing up to par.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to start a human evaluation que\n",
    "\n",
    "* Go to your test run\n",
    "* Select the test you want to evaluate\n",
    "* Filter if needed\n",
    "* Add to the annotation que\n",
    "* Go through each of the examples and provide the grade\n",
    "* Can create new tags for evaluation such as humor or creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Monitoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way to monitor LLM applications that currently in production.\n",
    "\n",
    "This is a current LLM application that is production that searches over the LangChain documentation.\n",
    "https://chat.langchain.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check for volume, Trace count, llm count ( how active)\n",
    "* Success Rates\n",
    "* Latency\n",
    "* Number of calls per trace\n",
    "* Tokens\n",
    "* Streaming metrics (how long until the first token is streamed back) (see how much the person is being taken care of)\n",
    "* Can see challenges and click in and see where that error happened.\n",
    "(This is very helpful way to monitor the agent in production in real time)\n",
    "\n",
    "* The aggregated feedback is posted up top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "* LangSmith is a great tool for debugging, evaluating, testing, and monitoring LLMs at any point during the production cycle.\n",
    "* Rich ecosystem with a ton of visibility and support through the LangChain Team.\n",
    "* Very powerful tool for anyone hoping to ship a LLM application and wants to be confident that the tools/agent is performing to standard.\n",
    "\n",
    "**Thank you for listening to my presentation**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
